{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05d9c386",
   "metadata": {},
   "source": [
    "# NeMo ASR to ONNX Conversion for Triton Inference Server\n",
    "\n",
    "## NeMo ASR ONNX Export\n",
    "\n",
    "This notebook guides you step-by-step through converting a pretrained NeMo ASR model (`nvidia/stt_en_fastconformer_ctc_large`) into optimized ONNX format modules:\n",
    "\n",
    "- **Preprocessor**: converts audio signal to Mel Spectrogram (kept in PyTorch due to limitations)\n",
    "- **ASR Acoustic Model**: generates logits from Mel Spectrogram\n",
    "- **CTC Decoder**: extracts text from logits (kept in PyTorch due to limitations)\n",
    "\n",
    "These modules are designed to be deployed independently using Triton Inference Server. Preprocessor and CTC decoder will use the PyTorch backend in Triton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc5bab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install git+https://github.com/NVIDIA/NeMo.git@v2.2.0rc3#egg=nemo_toolkit[asr]\n",
    "# !pip install onnxruntime-gpu==1.19.0 soundfile psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d11c2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import nemo.collections.asr as nemo_asr\n",
    "from nemo.collections.asr.modules import AudioToMelSpectrogramPreprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc8222e",
   "metadata": {},
   "source": [
    "## Step 1: Download and Initialize ASR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b92a0e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Device cuda or cpu\n",
    "device = 'cuda'\n",
    "model_name = \"nvidia/stt_en_fastconformer_ctc_large\"\n",
    "asr_model = nemo_asr.models.EncDecHybridRNNTCTCBPEModel.from_pretrained(\n",
    "    model_name=model_name, map_location='cuda'\n",
    ")\n",
    "asr_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55461d7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget https://dldata-public.s3.us-east-2.amazonaws.com/2086-149220-0033.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4badb2",
   "metadata": {},
   "source": [
    "## Step 2: Preprocessor Module\n",
    "Due to current limitations, we keep the Preprocessor as a PyTorch module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2343a9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = AudioToMelSpectrogramPreprocessor(features=80)\n",
    "preprocessor.to(device)\n",
    "\n",
    "# Test preprocessor\n",
    "audio, sr = sf.read('2086-149220-0033.wav')\n",
    "audio_array = np.array([audio])\n",
    "audio_signal = torch.from_numpy(audio_array).to(device)\n",
    "audio_signal_len = torch.tensor([audio_signal.shape[1]]).to(device)\n",
    "\n",
    "processed_signal, processed_signal_length = preprocessor(input_signal = audio_signal, \n",
    "                                                         length= audio_signal_len)\n",
    "print(processed_signal.shape, processed_signal_length.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6219e35c",
   "metadata": {},
   "source": [
    "## Step 3: Export ASR Acoustic Model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e86af20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceSTTEn(torch.nn.Module):\n",
    "    def __init__(self, model_inference):\n",
    "        super().__init__()\n",
    "        self.asr_model = model_inference\n",
    "\n",
    "    def forward(self, processed_signal):\n",
    "        return self.asr_model.forward_for_export(processed_signal)\n",
    "\n",
    "stt_module = InferenceSTTEn(asr_model)\n",
    "stt_module.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488dc3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.onnx.export(\n",
    "        stt_module,\n",
    "        processed_signal,\n",
    "        'model.onnx',\n",
    "        export_params=True,\n",
    "        input_names=[\"signal\"],\n",
    "        output_names=[\"output\"],\n",
    "        dynamic_axes={\n",
    "            \"signal\": {0: \"batch_size\", 2: \"sequence_length\"},\n",
    "            \"output\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c518786",
   "metadata": {},
   "source": [
    "## Step 4: CTC Decoder\n",
    "Due to current limitations, we keep the CTC decoder as a PyTorch module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02569769",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_model.decoding.strategy = 'greedy_batch'\n",
    "ctc_decoder = asr_model.decoding.ctc_decoder_predictions_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa8b679",
   "metadata": {},
   "source": [
    "## Step 5: ONNX Model Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95c8972",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "import psutil\n",
    "\n",
    "session_options = onnxruntime.SessionOptions()\n",
    "session_options.intra_op_num_threads = psutil.cpu_count(logical=True)\n",
    "session_options.log_severity_level = 1\n",
    "providers = [\"CUDAExecutionProvider\"]  # Change to CUDA if GPU is available\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession('model.onnx', session_options, providers);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7057ef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pipeline with ONNX model\n",
    "audio, sr = sf.read('2086-149220-0033.wav')\n",
    "\n",
    "audio_array = np.array([audio])\n",
    "audio_signal = torch.from_numpy(audio_array).to(device)\n",
    "audio_signal_len = torch.tensor([audio_signal.shape[1]]).to(device)\n",
    "\n",
    "processed_signal, processed_signal_length = preprocessor(input_signal = audio_signal, \n",
    "                                                         length= audio_signal_len)\n",
    "\n",
    "output = ort_session.run(None, {\"signal\": processed_signal.cpu().numpy()})\n",
    "\n",
    "pred_text = ctc_decoder(torch.from_numpy(output[0]))[0].text\n",
    "print(pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6345f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = asr_model.transcribe(['2086-149220-0033.wav'])[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e293cea1",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89197f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"NeMo text: {text}\")\n",
    "print(f\"ONNX text: {pred_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39aca86",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "\n",
    "Performance measured using %%timeit:\n",
    "\n",
    "- **Original PyTorch inference:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f1fbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10 -r 10\n",
    "text = asr_model.transcribe(['2086-149220-0033.wav'])[0].text\n",
    "\n",
    "### 96 ms ± 9.2 ms per loop (mean ± std. dev. of 10 runs, 10 loops each)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547ed7d9",
   "metadata": {},
   "source": [
    "- **ONNX optimized inference:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573e829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10 -r 10\n",
    "audio_array = np.array([audio])\n",
    "audio_signal = torch.from_numpy(audio_array).to(device)\n",
    "audio_signal_len = torch.tensor([audio_signal.shape[1]] * audio_array.shape[0]).to(device)\n",
    "\n",
    "processed_signal, processed_signal_length = preprocessor(input_signal = audio_signal, \n",
    "                                                         length= audio_signal_len)\n",
    "\n",
    "output = ort_session.run(None, {\"signal\": processed_signal.cpu().numpy()})\n",
    "\n",
    "pred_text = ctc_decoder(torch.from_numpy(output[0]), decoder_lengths=None)\n",
    "pred_text = [i.text for i in pred_text]\n",
    "\n",
    "### 16.7 ms ± 1.41 ms per loop (mean ± std. dev. of 10 runs, 10 loops each)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6039eea5",
   "metadata": {},
   "source": [
    "#### The optimized ONNX inference significantly outperforms the original PyTorch inference, delivering roughly **6x speed-up**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d180bbe",
   "metadata": {},
   "source": [
    "## Step 6: Packaging for Triton\n",
    "\n",
    "Follow instructions for Triton deployment:\n",
    "\n",
    "- [Triton Inference Server Documentation](https://github.com/triton-inference-server/server)\n",
    "- [Convert ONNX model to TensorRT](NeMo_convert_ONNX_to_TensorRT.md)\n",
    "\n",
    "\n",
    "## Next Steps\n",
    "- Deploy modules to Triton.\n",
    "- Benchmark performance.\n",
    "- Integrate modules into your application pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
